<html>
<head>
<title>GPTL timing library Home Page</title>
<meta name="description" content="Profiles multi-threaded and multi-tasked 
C, C++, and Fortran codes. Automatically generates a dynamic call tree">
<meta name="Keywords" content="gptl","papi","call tree","profile","timing","performance analysis">
<meta name="Author" content="Jim Rosinski">
<h1>GPTL - General Purpose Timing Library</h1>
<h2>(with optional PAPI interface)</h2>
</head>
<body bgcolor="peachpuff">

<b>GPTL</b> is a library to instrument C, C++, and Fortran codes for
    performance analysis and profiling. The instrumentation can be inserted
    manually by the user wherever they wish, or it can be done automatically
    by the compiler at
    function entry and exit points if the application being
    profiled is built 
    with either the GNU or PathScale compilers. To auto-instrument an
    application,
    add <b><em>-finstrument-functions</em></b> to 
    the compile flags of the source files to be profiled. Here is a portion of
    <b>GPTL</b> printout after running the HPCC benchmark with automatic
    instrumentation enabled: 
<pre>
<div style="background-color:white;">
Stats for thread 0:
                                             Called Recurse Wallclock max       min
  total                                            1    -      36.798    36.798    36.798
    HPCC_Init                                      5      4     0.020     0.020     0.000
*     HPL_pdinfo                                  58     56     0.012     0.012     0.000
*       HPL_all_reduce                             6    -       0.009     0.009     0.000
*         HPL_reduce                              17    -       0.009     0.009     0.000
            HPL_max                               17    -       0.000     0.000     0.000
            HPL_sum                                6    -       0.000     0.000     0.000
*         HPL_broadcast                           19    -       0.059     0.046     0.000
        HPL_pdlamch                                2    -       0.011     0.011     0.000
*       HPL_fprintf                              122    -       0.001     0.000     0.000
      HPCC_InputFileInit                           5      4     0.000     0.000     0.000
        ReadInts                                   2    -       0.000     0.000     0.000
    PTRANS                                        15     14    13.135    13.135     0.000
*     Cblacs_pinfo                                 7      1     0.000     0.000     0.000
      MaxMem                                       5      4     0.000     0.000     0.000
*       numroc_                                   96    -       0.000     0.000     0.000
*       iceil_                                   132    -       0.000     0.000     0.000
*       ilcm_                                     14    -       0.000     0.000     0.000
      Cblacs_get                                   5    -       0.000     0.000     0.000
*     setran_                                     25    -       0.000     0.000     0.000
*     pdrand                                 1.2e+06    -       9.262     0.056     0.000
*       HPL_lmul                             1.8e+06    -       2.065     0.040     0.000
*       HPL_ladd                             1.8e+06    -       2.265     0.056     0.000
      Cblacs_gridmap                              35     30     0.003     0.001     0.000
*     Cblacs_gridinfo                             60     50     0.000     0.000     0.000
...
</div>
</pre>
    Function names on the left of the output are indented to indicate their
    parent, and depth in the call tree. An asterisk next to an entry means it
    has more than one parent (see <A HREF="#EXAMPLE2">Example 2</A> below for
    further details). Other entries in this output show the number of
    invocations, number of recursive invocations, and wallclock timing
    statistics.
<p>
    If the <a HREF="http://icl.cs.utk.edu/papi">PAPI</a> library is
    installed on the target platform, <b>GPTL</b> can be used to
    access all available <b>PAPI</b> events.
    To count floating point operations for example, one need only add
    a call that looks like: 

    <pre>
    ret = GPTLsetoption (PAPI_FP_OPS, 1);
    </pre>

    The second argument "1" in the above call means "enable". Any non-zero
    integer means "enable", and a zero means "disable".
    Multiple <b>GPTL</b> or <b>PAPI</b> options can be specified with additional
    calls to <b>GPTLsetoption()</b>. The man pages provided with the
    distribution describe the full API specification for each <b>GPTL</b>
    function. The interface is identical for both Fortran and C/C++ 
    codes, modulo the case-insensitivity of Fortran. 
<p>
    Calls to <b>GPTLstart()</b> and <b>GPTLstop()</b> can be nested to an
    arbitrary depth. As shown above, <b>GPTL</b> handles nested regions by
    presenting output in an indented fashion. The example also shows how
    auto-instrumentation 
    can be used to easily produce a dynamic call tree of
    the application being profiled, where region names correspond to function
	entry and exit points.

<h2>Features</h2>
<ul>
<li> Low overhead.
<li> No external dependencies (<b>PAPI</b> interface is optional).
<li> Automatically multiplexes requested <b>PAPI</b> counters when required.
<li> Thread-safe, and reports per-thread statistics for multi-threaded
	codes.
<li> Includes utility functions to print memory usage
	(<b>GPTLprint_memusage()</b>) and get timestamps (<b>GPTLstamp()</b>).
<li> Includes utility scripts to post-process multi-threaded and
	multi-tasked output for easy assessment of load balance
	characteristics.
<li> Support for derived (<b>PAPI</b>-based) events such as computational
	intensity and instructions per cycle.
</ul>

<h2>Download and Installation</h2>
<ul> 
<li> Download the most recent release by clicking this link: <a href="gptl3_4_1.tar.gz">gptl3_4_1.tar.gz</a>
<li> Earlier versions can be found <a href="older/">here</a>
<li> To build and install <b>GPTL</b>, see the <A HREF="INSTALL">INSTALL</a> file.
  You'll need to create a <b><em>macros.make</em></b> file appropriate for your target
  platform. Example files for various architectures are
  included in the tar file (e.g. <b><em>macros.make.linux</em></b>). An autoconf-based 
  script named <b><em>suggestions</em></b> is included to help in editing this
  file. Example usage might be:
<pre>
./suggestions FC=gfortran CC=gcc
</pre>
Comments in the sample <b><em>macros.make</em></b> files describe 
  each required setting.
<li> For information on using <b>GPTL</b>, refer to 
  <A HREF="#EXAMPLES">EXAMPLES</A> below, and the man pages provided with the
  distribution. 
</ul>

<HR SIZE=2 WIDTH="100%" ALIGN="LEFT" NOSHADE>
<A name=EXAMPLES></A>
<h2>Examples</h2>
  Below are three simple codes which illustrate the use of some features of
  <b>GPTL</b>. They were all run on a Linux x86 using GNU compilers. 
  <A HREF="#EXAMPLE1">Example 1</A> is a manually-instrumented
  Fortran code which uses <b>PAPI</b> to count floating point
  operations. <A HREF="#EXAMPLE2">Example 2</A> is C code compiled
  with gcc's auto-instrumentation hooks to print a dynamic call tree. 
  <A HREF="#EXAMPLE3">Example 3</A> is a simple MPI code, the
  output of which is post-processed using Perl script
  <em><b>parsegpltout.pl</b></em> to examine load imbalance.

<HR SIZE=2 WIDTH="100%" ALIGN="LEFT" NOSHADE>
<A name=EXAMPLE1></A>
<h3>Example 1: Simple manual instrumentation</h3>
This is an OpenMP Fortran
	code manually instrumented with <b>GPTL</b> calls. The 
	output produced by the embedded call to <b>gptlpr()</b> is
	then shown and explained.
<p>
<b><em>papiomptest.f90:</em></b>
<pre>
<div style="background-color:white;">
program papiomptest
  implicit none
  include 'gptl.inc'                 ! Fortran GPTL include file
  include 'f90papi.h'                ! Needed for PAPI_FP_OPS
  integer :: ret, iter
  integer, parameter :: nompiter = 2 ! Number of OMP threads

  ret = gptlsetoption (gptlabort_on_error, 1) ! Abort on GPTL error
  ret = gptlsetoption (PAPI_FP_OPS, 1)        ! Count floating point ops
  ret = gptlsetoption (gptlnarrowprint, 1)    ! Print fewer sig figs
  ret = gptlsetoption (gptlpercent, 1)        ! Turn on "% of" print
  ret = gptlsetoption (gptloverhead, 0)       ! Turn off overhead estimate
  ret = gptlinitialize ()                     ! Initialize GPTL
  ret = gptlstart ('total')                   ! Start a timer

!$OMP PARALLEL DO PRIVATE (iter)   ! Threaded loop
  do iter=1,nompiter
    ret = gptlstart ('A')          ! Start a timer
    ret = gptlstart ('B')          ! Start another timer
    ret = gptlstart ('C')
    call sleep (iter)              ! Sleep for "iter" seconds
    ret = gptlstop ('C')           ! Stop a timer
    ret = gptlstart ('CC')
    ret = gptlstop ('CC')
    ret = gptlstop ('A')
    ret = gptlstop ('B')         
  end do
  ret = gptlstop ('total')
  ret = gptlpr (0)                 ! Print timer stats
  ret = gptlfinalize ()            ! Clean up
end program papiomptest
</div>
</pre>

Compile and link, then run:
<pre>
% gfortran -fopenmp papiomptest.f90 -I/usr/local/include -lgptl -lpapi 
% env OMP_NUM_THREADS=2 ./a.out
</pre>

The call to <b>gptlpr(0)</b> wrote a file named timing.0, which looks like this:

<pre>
<div style="background-color:white;">
PAPI event multiplexing was OFF
PAPI events enabled (including derived):
  Floating point operations executed

Underlying timing routine was gettimeofday.
Per-call utr overhead est: 2.9e-07 sec.
Per-call PAPI overhead est: 1.4e-07 sec.
If overhead stats are printed, roughly half the estimated number is
embedded in the wallclock (and/or PAPI counter) stats for each timer

If a '% of' field is present, it is w.r.t. the first timer for thread 0.
If a 'e6 per sec' field is present, it is in millions of PAPI counts per sec.

A '*' in column 1 below means the timer had multiple parents, though the
values printed are for all calls. Further down the listing is more detailed
information about multiple parents. Look for 'Multiple parent info'

Stats for thread 0:
             Called Recurse Wallclock max       min       % of total   FP_OPS e6 / sec 
  total             1   -       2.000     2.000     2.000     100.00       59     0.00 
    A               1   -       1.000     1.000     1.000      50.00       32     0.00 
      B             1   -       1.000     1.000     1.000      50.00       36     0.00 
        C           1   -       1.000     1.000     1.000      50.00        4     0.00 
        CC          1   -       0.000     0.000     0.000       0.00        4     4.00 
Total calls           = 5
Total recursive calls = 0

Stats for thread 1:
        Called Recurse Wallclock max       min       % of total   FP_OPS e6 / sec 
  A            1   -       2.000     2.000     2.000     100.00       50     0.00 
    B          1   -       2.000     2.000     2.000     100.00       54     0.00 
      C        1   -       2.000     2.000     2.000     100.00       22     0.00 
      CC       1   -       0.000     0.000     0.000       0.00        4     4.00 
Total calls           = 4
Total recursive calls = 0

Same stats sorted by timer for threaded regions:
Thd      Called Recurse Wallclock max       min       % of total   FP_OPS e6 / sec 
000 A           1   -       1.000     1.000     1.000      50.00       32     0.00 
001 A           1   -       2.000     2.000     2.000     100.00       50     0.00 
SUM A           2   -       3.000     2.000     1.000     150.00       82     0.00 

000 B           1   -       1.000     1.000     1.000      50.00       36     0.00 
001 B           1   -       2.000     2.000     2.000     100.00       54     0.00 
SUM B           2   -       3.000     2.000     1.000     150.00       90     0.00 

000 C           1   -       1.000     1.000     1.000      50.00        4     0.00 
001 C           1   -       2.000     2.000     2.000     100.00       22     0.00 
SUM C           2   -       3.000     2.000     1.000     150.00       26     0.00 

000 CC          1   -       0.000     0.000     0.000       0.00        4     4.00 
001 CC          1   -       0.000     0.000     0.000       0.00        4     4.00 
SUM CC          2   -       0.000     0.000     0.000       0.00        8     4.00 
</div>
</pre>

<h3>Explanation of the above output</h3>
The output file contains a preamble which lists <b>PAPI</b>
settings such as whether multiplexing was on or off, and
which <b>PAPI</b> events were enabled. In this case 
"Floating point operations executed" were counted. Other preamble contents
include estimates of underlying timing routine (UTR)
overhead, <b>PAPI</b> overhead, and an explanation of the printed
statistics. 
<p>
The statistics themselves begin with the line which reads "Stats for
thread 0:". The region names are listed on the far left. A 
"region" is defined in the application by calling
<b>GPTLstart()</b>, then <b>GPTLstop()</b> for the same input (character
string) argument.
Indenting of 
the names preserves parent-child relationships between the regions. In
the example, we see that region "A" was contained in "total", "B"
contained in "A", and regions "C" and "CC" both contained in "B". 
<p>
Reading across the output from left to right, the next column is labelled
"Called". This is the number of times the region was invoked. If any regions
were called recursively, that information is printed next. In this case there
were no recursive calls, so just a "-" is printed. Total wallclock time for
each region is printed next, followed by the max and min values for any
single invocation. In this simple example each region was called only once, so
"Wallclock", "max", and "min" are all the same. The next column lists the
percentage of wallclock time each region took compared to the first
region timed, and was produced due to the call to <b>GPTLsetoption (GPTLpercent,1)</b>. 
Turning this option on is generally useful only if there is a single region
wrapping the entire execution ("total" in the above example).
<b>PAPI</b>-based statistics are presented next. In
the example, the counter PAPI_FP_OPS was enabled. The name was shortened to FP_OPS to
confine the printed output to as few columns as possible. Finally, each <b>PAPI</b>
count is divided by wallclock time and printed as millions per second (in
this case millions of floating point operations per second). This column can
be turned off, with a call to <b>GPTLsetoption (GPTLpersec, 0)</b>. 

<p>
Since this was a threaded code run with OMP_NUM_THREADS=2, statistics
for the second thread are also printed. It starts at "Stats for thread 1:" The
output shows that thread 1
participated in the computations for regions "A", "B", "C", and "CC", but not
"total". This is reflected in the code itself, since only the master
thread was active when start and stop calls were made for region "total".

<p>
After the per-thread statistics section, the same information is repeated, sorted by
region name if more than one thread was active. This section is delimited by
the string "Same stats sorted by
timer for threaded regions:". This region presentation order makes it easier
	to inspect for load 
balance across threads. The leftmost column is thread number, and the region
names are not indented. A sum across threads for each region is also printed,
and labeled "SUM".

<HR SIZE=2 WIDTH="100%" ALIGN="LEFT" NOSHADE>

<A name=EXAMPLE2></A>
<h3>Example 2: Auto-instrumentation</h3>
The next example is a C code compiled with auto-instrumentation enabled. It
uses <b>PAPI</b> to count total instructions, and instructions per cycle. Note
that function <em>B</em> has multiple parents, and <b>GPTL</b> reports the
	multiple parent information in the output produced by the call
	to <b>GPTLpr_file()</b>. 
<p>
<b><em>main.c:</em></b>
<pre>
<div style="background-color:white;">
#include &#60gptl.h&gt
#include &#60papi.h&gt

int main ()
{
  void do_work (void);
  int i, ret;
  ret = GPTLsetoption (GPTL_IPC, 1);     // Count instructions per cycle
  ret = GPTLsetoption (PAPI_TOT_INS, 1); // Print total instructions
  ret = GPTLsetoption (GPTLoverhead, 0); // Don't print overhead estimate
  ret = GPTLinitialize ();               // Initialize GPTL
  ret = GPTLstart ("main");              // Start a manual timer
  do_work ();                            // Do some work 
  ret = GPTLstop ("main");               // Stop the manual timer
  ret = GPTLpr_file ("outfile");         // Write output to "outfile"
}
</div>
</pre>

<b><em>subs.c:</em></b>
<div style="background-color:white;">
<pre>
#include &#60unistd.h&gt

extern void A(void);
extern void AA(void);
extern void B(void);

void do_work ()
{
  A ();
  AA ();
  B ();
}

void A ()
{
  B ();
}

void AA ()
{
}

void B ()
{
  sleep (1);
}
</div>
</pre>
Compile all but <em>main.c</em> with auto-instrumentation, then link and
run. Useful auto-instrumentation of the main program is not possible,
because the call to <b>GPTLinitialize()</b> must be done manually and
needs to preceed all calls to <b>GPTLstart</b> and <b>GPTLstop</b>. 
<pre>
% gcc -c main.c
% gcc -finstrument-functions subs.c main.o -lgptl -lpapi
% ./a.out
</pre>

Now convert the auto-instrumented output to human-readable form:
<pre>
% hex2name.pl a.out outfile > outfile.converted
</pre>

Output file <em> outfile.converted</em> looks like this:
<pre>
<div style="background-color:white;">
PAPI event multiplexing was OFF
PAPI events enabled (including those required for derived events):
  Instructions per cycle
  Total instructions executed

Underlying timing routine was gettimeofday.
Per-call utr overhead est: 2.8e-07 sec.
Per-call PAPI overhead est: 1.4e-07 sec.
If overhead stats are printed, roughly half the estimated number is
embedded in the wallclock stats for each timer

If a '% of' field is present, it is w.r.t. the first timer for thread 0.
If a 'e6 per sec' field is present, it is in millions of PAPI counts per sec.

A '*' in column 1 below means the timer had multiple parents, though the
values printed are for all calls. Further down the listing is more detailed
information about multiple parents. Look for 'Multiple parent info'

Stats for thread 0:
                     Called Recurse Wallclock max       min       GPTL_IPC  TOT_INS e6 / sec 
  main                     1    -       2.000     2.000     2.000 1.35e-01    16575     0.01 
    do_work                1    -       2.000     2.000     2.000 1.24e-01    12221     0.01 
      A                    1    -       1.000     1.000     1.000 1.33e-01     4890     0.00 
*       B                  2    -       2.000     1.000     1.000 5.23e-02     2739     0.00 
      AA                   1    -       0.000     0.000     0.000 4.46e-01      429   429.00 
Total calls           = 6
Total recursive calls = 0

Multiple parent info (if any) for thread 0:
Columns are count and name for the listed child
Rows are each parent, with their common child being the last entry, which is indented
Count next to each parent is the number of times it called the child
Count next to child is total number of times it was called by the listed parents

       1 A                         
       1 do_work                         
       2   B                         
</div>
</pre>
<h3>Explanation of the above output</h3>
<b>PAPI</b> event "Total instructions executed"
(PAPI_TOT_INS) and derived event "Instructions per
cycle" (GPTL_IPC) were enabled. To compute instructions per
cycle, <b>GPTL</b> made the 
<b>PAPI</b> library call to count total cycles (PAPI_TOT_CYC) in addition to
the already-enabled event PAPI_TOT_INS. When
<b>GPTLpr_file()</b> was called, it computed:
<pre>
      GPTL_IPC = PAPI_TOT_INS / PAPI_TOT_CYC;
</pre>
<p>
Note the asterisk in front of region "B". This
indicates that region "B" had multiple parents. It is presented as a child of
region "A" because that is the first region that invoked it. Information
about other parents is presented after the main call tree. It shows that
region "B" had two parents, "A", and "do_work". Each parent invoked "B" once,
for a total of 2 calls.

<HR SIZE=2 WIDTH="100%" ALIGN="LEFT" NOSHADE>

<A name=EXAMPLE3></A>
<h3>Example 3: Load imbalance</h3>
This hybrid OpenMP/MPI code simulates load imbalance at both the
	thread and task level by sleeping a number of seconds equal to
	the thread number plus the MPI rank, within a region being
	timed. After running the code, we use utility script 
	<em><b>parsegptlout.pl</b></em> 
	to examine timing statistics for the region.
<p>
<b><em>mpi.c:</em></b>
<pre>
<div style="background-color:white;">
#include &#60unistd.h&gt
#include &#60omp.h&gt
#include &#60mpi.h&gt
#include &#60gptl.h&gt

int main (int argc, char **argv)
{
  int i, ret, iam;
  int nompiter = omp_get_max_threads ();       // Loop trip count matches number of threads

  ret = MPI_Init (&argc, &argv);               // Initialize MPI
  ret = MPI_Comm_rank (MPI_COMM_WORLD, &iam);  // Get my rank

  ret = GPTLsetoption (GPTLoverhead, 0);       // Don't print overhead stats
  ret = GPTLsetoption (GPTLpercent, 1);        // Print percentage stats
  ret = GPTLsetoption (GPTLabort_on_error, 1); // Abort on any GPTL error

  ret = GPTLinitialize ();                     // Initialize GPTL
  ret = GPTLstart ("total");                   // Time the whole program

  /* Threaded loop with load imbalance across both OMP threads and MPI tasks */

#pragma omp parallel for private (i, ret)
  for (i = 0; i < nompiter; i++) {
    ret = GPTLstart ("sleep_iam_plus_mythread");// Start timer for simulated work
    ret = sleep (iam+i);                        // Load-imbalanced work
    ret = GPTLstop ("sleep_iam_plus_mythread"); // Stop timer for simulated work
  }

  ret = GPTLstart ("barriersync");             // Time MPI task synchronization
  ret = MPI_Barrier (MPI_COMM_WORLD);          // Synchronize MPI tasks
  ret = GPTLstop ("barriersync");

  ret = GPTLstart ("sleep_1");                // This region has a balanced load
  ret = sleep (1);
  ret = GPTLstop ("sleep_1");
  ret = GPTLstop ("total");

  ret = GPTLpr (iam);                          // Print the results
  ret = MPI_Finalize ();                       // Clean up MPI
  return 0;
}
</div>
</pre>

Compile and link, then run with 2 threads and 3 MPI tasks:
<pre>
% gcc -fopenmp mpi.c -lmpich -lgptl
% env OMP_NUM_THREADS=2 mpiexec -n 3 ./a.out
</pre>

Output files <em>timing.0</em>, <em>timing.1</em>, and <em>timing.2</em> are created by the call
to <b>GPTLpr()</b>, one for each MPI task. First let's examine the <b>GPTL</b> output
file for task 0. Preamble and postamble print has been deleted for
brevity.
<p>
<b><em>timing.0:</em></b>
<pre>
<div style="background-color:white;">
Stats for thread 0:
                           Called Recurse Wallclock max       min       % of total 
  total                           1   -       4.003     4.003     4.003     100.00 
    sleep_iam_plus_mythread       1   -       0.000     0.000     0.000       0.00 
    barriersync                   1   -       2.001     2.001     2.001      49.99 
    sleep_1                       1   -       1.000     1.000     1.000      24.99 
Total calls           = 4
Total recursive calls = 0

Stats for thread 1:
                         Called Recurse Wallclock max       min       % of total 
  sleep_iam_plus_mythread       1   -       1.000     1.000     1.000      24.98 
Total calls           = 1
Total recursive calls = 0

Same stats sorted by timer for threaded regions:
Thd                        Called Recurse Wallclock max       min       % of total 
000 sleep_iam_plus_mythread       1   -       0.000     0.000     0.000       0.00 
001 sleep_iam_plus_mythread       1   -       1.000     1.000     1.000      24.98 
SUM sleep_iam_plus_mythread       2   -       1.000     1.000     0.000      24.98 
</div>
</pre>

The results are fairly straightforward and mostly as expected. Note
though that for thread 0, region "total" took 4 seconds, while
the sum of all the other regions it surrounded adds up to only
3. What's going on? The reason is that
there is an implied (and untimed) OpenMP barrier before the end of the threaded loop.
Thread 0 slept 0 seconds, then had to wait for thread 1 to finish
sleeping for 1 second before it exited the loop.
<p>
Next we'll use <em><b>parsegpltout.pl</b></em> to examine the
timing files for all MPI tasks and gather load balance statistics for one of the regions
being timed:
<pre>
% parsegptlout.pl sleep_iam_plus_mythread
</pre>
Here's the output:
<pre>
<div style="background-color:white;">
Searched for region sleep_iam_plus_mythread
Found 6 calls across 3 tasks and 2 threads per task
6 of a possible 6 tasks and threads had entries for sleep_iam_plus_mythread
Heading is Wallclock
Max   =  3.000 on thread 1 task 2
Min   =  0.000 on thread 0 task 0
Mean  = 1.5
Total = 9
</div>
</pre>
We see that the max time for this region was spent in thread 1, task
2 (though only task 0 output is shown above). The three seconds reported is due to the call
to <b>sleep (iam+i)</b>. Likewise, thread 0 on task 0 spent
the least amount of time in this region, 0 seconds.

<HR SIZE=2 WIDTH="100%" ALIGN="LEFT" NOSHADE>

<h2>Bug Reports</h2>
Please <a HREF="mailto:rosinskijm@ornl.gov">email</a> me bug reports
	and/or feature requests.

<h2>Author</h2>
<b>GPTL</b> was written
by <a HREF="http://www.burningserver.net/rosinski">Jim Rosinski</a>,
currently at <a HREF="http://www.ornl.gov">ORNL</a>, formerly
  at <a HREF="http://www.sicortex.com">SiCortex</a>,
and <a HREF="http://www.ucar.edu">NCAR</a>. 
<h2>Copyright</h2>
This software is <b>Open Source</b>. My only request is that you don't
embed <b>GPTL</b> library source itself in software that you intend to sell.
</html>
